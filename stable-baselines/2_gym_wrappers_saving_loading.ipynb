{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_gym_wrappers_saving_loading.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ezJ3Y7XRUnj"
      },
      "source": [
        "# Stable Baselines3 Tutorial - Gym wrappers, saving and loading models\n",
        "\n",
        "Github repo: https://github.com/araffin/rl-tutorial-jnrr19/tree/sb3/\n",
        "\n",
        "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "Documentation: https://stable-baselines3.readthedocs.io/en/master/\n",
        "\n",
        "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you will learn how to use *Gym Wrappers* which allow to do monitoring, normalization, limit the number of steps, feature augmentation, ...\n",
        "\n",
        "\n",
        "You will also see the *loading* and *saving* functions, and how to read the outputed files for possible exporting.\n",
        "\n",
        "## Install Dependencies and Stable Baselines3 Using Pip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFdlFByORUnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "970c5adb-a5e5-47f5-b18b-beed6c8bb86a"
      },
      "source": [
        "!apt install swig\n",
        "!pip install stable-baselines3[extra]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 1,100 kB of archives.\n",
            "After this operation, 5,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,100 kB in 0s (9,050 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 155219 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-1.3.0-py3-none-any.whl (174 kB)\n",
            "\u001b[K     |████████████████████████████████| 174 kB 13.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.2.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.9.0+cu111)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.1.5)\n",
            "Requirement already satisfied: gym<0.20,>=0.17 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.17.3)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.2.9)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.1.2.30)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (2.6.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym<0.20,>=0.17->stable-baselines3[extra]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym<0.20,>=0.17->stable-baselines3[extra]) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.20,>=0.17->stable-baselines3[extra]) (0.16.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.12.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.37.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.41.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->stable-baselines3[extra]) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->stable-baselines3[extra]) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->stable-baselines3[extra]) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2018.9)\n",
            "Installing collected packages: stable-baselines3\n",
            "Successfully installed stable-baselines3-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grXe85G9RUnp"
      },
      "source": [
        "import gym\n",
        "from stable_baselines3 import A2C, SAC, PPO, TD3"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMPAn1SRd32f"
      },
      "source": [
        "# Saving and loading\n",
        "\n",
        "Saving and loading stable-baselines models is straightforward: you can directly call `.save()` and `.load()` on the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBNFnN4Gd32g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d469738-232a-4a1b-8247-102f1f246c0d"
      },
      "source": [
        "import os\n",
        "\n",
        "# Create save dir\n",
        "save_dir = \"/tmp/gym/\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "model = PPO('MlpPolicy', 'Pendulum-v0', verbose=0).learn(8000)\n",
        "# The model will be saved under PPO_tutorial.zip\n",
        "model.save(save_dir + \"/PPO_tutorial\")\n",
        "\n",
        "# sample an observation from the environment\n",
        "obs = model.env.observation_space.sample()\n",
        "\n",
        "# Check prediction before saving\n",
        "print(\"pre saved\", model.predict(obs, deterministic=True))\n",
        "\n",
        "del model # delete trained model to demonstrate loading\n",
        "\n",
        "loaded_model = PPO.load(save_dir + \"/PPO_tutorial\")\n",
        "# Check that the prediction is the same after loading (for the same observation)\n",
        "print(\"loaded\", loaded_model.predict(obs, deterministic=True))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pre saved (array([-0.05292664], dtype=float32), None)\n",
            "loaded (array([-0.05292664], dtype=float32), None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXWPrVqId32o"
      },
      "source": [
        "Saving in stable-baselines is quite powerful, as you save the training hyperparameters, with the current weights. This means in practice, you can simply load a custom model, without redefining the parameters, and continue learning.\n",
        "\n",
        "The loading function can also update the model's class variables when loading."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCtxrAbXd32q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dec046e-27f4-4555-97e7-bbf23f9afc09"
      },
      "source": [
        "import os\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Create save dir\n",
        "save_dir = \"/tmp/gym/\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "model = A2C('MlpPolicy', 'Pendulum-v0', verbose=0, gamma=0.9, n_steps=20).learn(8000)\n",
        "# The model will be saved under A2C_tutorial.zip\n",
        "model.save(save_dir + \"/A2C_tutorial\")\n",
        "\n",
        "del model # delete trained model to demonstrate loading\n",
        "\n",
        "# load the model, and when loading set verbose to 1\n",
        "loaded_model = A2C.load(save_dir + \"/A2C_tutorial\", verbose=1)\n",
        "\n",
        "# show the save hyperparameters\n",
        "print(\"loaded:\", \"gamma =\", loaded_model.gamma, \"n_steps =\", loaded_model.n_steps)\n",
        "\n",
        "# as the environment is not serializable, we need to set a new instance of the environment\n",
        "loaded_model.set_env(DummyVecEnv([lambda: gym.make('Pendulum-v0')]))\n",
        "# and continue training\n",
        "loaded_model.learn(8000)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded: gamma = 0.9 n_steps = 20\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1532     |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.42    |\n",
            "|    explained_variance | 0.000453 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | -39.3    |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 809      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1535      |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.42     |\n",
            "|    explained_variance | -8.11e-06 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -60.9     |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.35e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1548      |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.42     |\n",
            "|    explained_variance | -0.000101 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -37.2     |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 966       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1544     |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.43    |\n",
            "|    explained_variance | 3.58e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | -40.1    |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 1.04e+03 |\n",
            "------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.a2c.a2c.A2C at 0x7f7c2cc7c950>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKwupU-Jgxjm"
      },
      "source": [
        "# Gym and VecEnv wrappers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds4AAfmISQIA"
      },
      "source": [
        "## Anatomy of a gym wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnTS9e9hTzZZ"
      },
      "source": [
        "A gym wrapper follows the [gym](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html) interface: it has a `reset()` and `step()` method.\n",
        "\n",
        "Because a wrapper is *around* an environment, we can access it with `self.env`, this allow to easily interact with it without modifying the original env.\n",
        "There are many wrappers that have been predefined, for a complete list refer to [gym documentation](https://github.com/openai/gym/tree/master/gym/wrappers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYo0C0TQSL3c"
      },
      "source": [
        "class CustomWrapper(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  :param env: (gym.Env) Gym environment that will be wrapped\n",
        "  \"\"\"\n",
        "  def __init__(self, env):\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super(CustomWrapper, self).__init__(env)\n",
        "  \n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment \n",
        "    \"\"\"\n",
        "    obs = self.env.reset()\n",
        "    return obs\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    :param action: ([float] or int) Action taken by the agent\n",
        "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
        "    \"\"\"\n",
        "    obs, reward, done, info = self.env.step(action)\n",
        "    return obs, reward, done, info\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zeGuyICUN26"
      },
      "source": [
        "## First example: limit the episode length\n",
        "\n",
        "One practical use case of a wrapper is when you want to limit the number of steps by episode, for that you will need to overwrite the `done` signal when the limit is reached. It is also a good practice to pass that information in the `info` dictionnary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb2U4_K6SNUx"
      },
      "source": [
        "class TimeLimitWrapper(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  :param env: (gym.Env) Gym environment that will be wrapped\n",
        "  :param max_steps: (int) Max number of steps per episode\n",
        "  \"\"\"\n",
        "  def __init__(self, env, max_steps=100):\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super(TimeLimitWrapper, self).__init__(env)\n",
        "    self.max_steps = max_steps\n",
        "    # Counter of steps per episode\n",
        "    self.current_step = 0\n",
        "  \n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment \n",
        "    \"\"\"\n",
        "    # Reset the counter\n",
        "    self.current_step = 0\n",
        "    return self.env.reset()\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    :param action: ([float] or int) Action taken by the agent\n",
        "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
        "    \"\"\"\n",
        "    self.current_step += 1\n",
        "    obs, reward, done, info = self.env.step(action)\n",
        "    # Overwrite the done signal when \n",
        "    if self.current_step >= self.max_steps:\n",
        "      done = True\n",
        "      # Update the info dict to signal that the limit was exceeded\n",
        "      info['time_limit_reached'] = True\n",
        "    return obs, reward, done, info\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZufaUJwVM9w"
      },
      "source": [
        "#### Test the wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szZ43D5PVB07"
      },
      "source": [
        "from gym.envs.classic_control.pendulum import PendulumEnv\n",
        "\n",
        "# Here we create the environment directly because gym.make() already wrap the environement in a TimeLimit wrapper otherwise\n",
        "env = PendulumEnv()\n",
        "# Wrap the environment\n",
        "env = TimeLimitWrapper(env, max_steps=100)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cencka9iVg9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02728616-4aa7-4445-9e0c-71bb4169eb35"
      },
      "source": [
        "obs = env.reset()\n",
        "done = False\n",
        "n_steps = 0\n",
        "while not done:\n",
        "  # Take random actions\n",
        "  random_action = env.action_space.sample()\n",
        "  obs, reward, done, info = env.step(random_action)\n",
        "  n_steps += 1\n",
        "\n",
        "print(n_steps, info)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 {'time_limit_reached': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkMYA63sV9aA"
      },
      "source": [
        "In practice, `gym` already have a wrapper for that named `TimeLimit` (`gym.wrappers.TimeLimit`) that is used by most environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIIJbSyQW9R-"
      },
      "source": [
        "## Second example: normalize actions\n",
        "\n",
        "It is usually a good idea to normalize observations and actions before giving it to the agent, this prevent [hard to debug issue](https://github.com/hill-a/stable-baselines/issues/473).\n",
        "\n",
        "In this example, we are going to normalize the action space of *Pendulum-v0* so it lies in [-1, 1] instead of [-2, 2].\n",
        "\n",
        "Note: here we are dealing with continuous actions, hence the `gym.Box` space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5E6kZfzW8vy"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class NormalizeActionWrapper(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  :param env: (gym.Env) Gym environment that will be wrapped\n",
        "  \"\"\"\n",
        "  def __init__(self, env):\n",
        "    # Retrieve the action space\n",
        "    action_space = env.action_space\n",
        "    assert isinstance(action_space, gym.spaces.Box), \"This wrapper only works with continuous action space (spaces.Box)\"\n",
        "    # Retrieve the max/min values\n",
        "    self.low, self.high = action_space.low, action_space.high\n",
        "\n",
        "    # We modify the action space, so all actions will lie in [-1, 1]\n",
        "    env.action_space = gym.spaces.Box(low=-1, high=1, shape=action_space.shape, dtype=np.float32)\n",
        "\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super(NormalizeActionWrapper, self).__init__(env)\n",
        "  \n",
        "  def rescale_action(self, scaled_action):\n",
        "      \"\"\"\n",
        "      Rescale the action from [-1, 1] to [low, high]\n",
        "      (no need for symmetric action space)\n",
        "      :param scaled_action: (np.ndarray)\n",
        "      :return: (np.ndarray)\n",
        "      \"\"\"\n",
        "      return self.low + (0.5 * (scaled_action + 1.0) * (self.high -  self.low))\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment \n",
        "    \"\"\"\n",
        "    # Reset the counter\n",
        "    return self.env.reset()\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    :param action: ([float] or int) Action taken by the agent\n",
        "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
        "    \"\"\"\n",
        "    # Rescale action from [-1, 1] to original [low, high] interval\n",
        "    rescaled_action = self.rescale_action(action)\n",
        "    obs, reward, done, info = self.env.step(rescaled_action)\n",
        "    return obs, reward, done, info\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmJ0eahNaR6K"
      },
      "source": [
        "#### Test before rescaling actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEnjBwisaQIx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4331d2f-3b07-4514-c8b3-5da77b13e743"
      },
      "source": [
        "original_env = gym.make(\"Pendulum-v0\")\n",
        "\n",
        "print(original_env.action_space.low)\n",
        "for _ in range(10):\n",
        "  print(original_env.action_space.sample())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-2.]\n",
            "[-1.8865008]\n",
            "[0.76111114]\n",
            "[-1.2962222]\n",
            "[-0.14304268]\n",
            "[0.7447886]\n",
            "[-1.4128095]\n",
            "[-1.5202788]\n",
            "[1.5071152]\n",
            "[-1.0076494]\n",
            "[1.4562893]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvcll2L3afVd"
      },
      "source": [
        "#### Test the NormalizeAction wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsCM9AUGaeBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8509458f-2543-4872-c43d-08fd591bff87"
      },
      "source": [
        "env = NormalizeActionWrapper(gym.make(\"Pendulum-v0\"))\n",
        "\n",
        "print(env.action_space.low)\n",
        "\n",
        "for _ in range(10):\n",
        "  print(env.action_space.sample())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.]\n",
            "[-0.7095748]\n",
            "[0.02797176]\n",
            "[0.5938129]\n",
            "[-0.01302343]\n",
            "[0.17248467]\n",
            "[-0.6452554]\n",
            "[0.94569093]\n",
            "[-0.5845174]\n",
            "[0.6165716]\n",
            "[-0.11070762]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5h5kk2mbGNs"
      },
      "source": [
        "#### Test with a RL algorithm\n",
        "\n",
        "We are going to use the Monitor wrapper of stable baselines, wich allow to monitor training stats (mean episode reward, mean episode length)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9FNCN8ybOVU"
      },
      "source": [
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wutM3c1GbfGP"
      },
      "source": [
        "env = Monitor(gym.make('Pendulum-v0'))\n",
        "env = DummyVecEnv([lambda: env])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cxnE5bdaQ_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "721e02de-83b4-4154-a97f-dae87d6e93b2"
      },
      "source": [
        "model = A2C(\"MlpPolicy\", env, verbose=1).learn(int(1000))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 200       |\n",
            "|    ep_rew_mean        | -1.47e+03 |\n",
            "| time/                 |           |\n",
            "|    fps                | 920       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.44     |\n",
            "|    explained_variance | -0.00501  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | -39.1     |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 1.34e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 200       |\n",
            "|    ep_rew_mean        | -1.37e+03 |\n",
            "| time/                 |           |\n",
            "|    fps                | 932       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.44     |\n",
            "|    explained_variance | -0.00725  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | -41.2     |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 826       |\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJFSM-Drb3Wc"
      },
      "source": [
        "With the action wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GszFZthob2wM"
      },
      "source": [
        "normalized_env = Monitor(gym.make('Pendulum-v0'))\n",
        "# Note that we can use multiple wrappers\n",
        "normalized_env = NormalizeActionWrapper(normalized_env)\n",
        "normalized_env = DummyVecEnv([lambda: normalized_env])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrKJEO4NcIMd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b42e95e-008c-4c03-e10d-28c8f93bc3a6"
      },
      "source": [
        "model_2 = A2C(\"MlpPolicy\", normalized_env, verbose=1).learn(int(1000))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 200       |\n",
            "|    ep_rew_mean        | -1.09e+03 |\n",
            "| time/                 |           |\n",
            "|    fps                | 929       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.42     |\n",
            "|    explained_variance | 0.0341    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | -4.01     |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 7.26      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 200       |\n",
            "|    ep_rew_mean        | -1.25e+03 |\n",
            "| time/                 |           |\n",
            "|    fps                | 924       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.41     |\n",
            "|    explained_variance | -0.0288   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | -35.6     |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 909       |\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BxqXd_6dpJx"
      },
      "source": [
        "## Additional wrappers: VecEnvWrappers\n",
        "\n",
        "In the same vein as gym wrappers, stable baselines provide wrappers for `VecEnv`. Among the different that exist (and you can create your own), you should know: \n",
        "\n",
        "- VecNormalize: it computes a running mean and standard deviation to normalize observation and returns\n",
        "- VecFrameStack: it stacks several consecutive observations (useful to integrate time in the observation, e.g. sucessive frame of an atari game)\n",
        "\n",
        "More info in the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#wrappers)\n",
        "\n",
        "Note: when using `VecNormalize` wrapper, you must save the running mean and std along with the model, otherwise you will not get proper results when loading the agent again. If you use the [rl zoo](https://github.com/DLR-RM/rl-baselines3-zoo), this is done automatically"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuIcbfv3g9dd"
      },
      "source": [
        "from stable_baselines3.common.vec_env import VecNormalize, VecFrameStack\n",
        "\n",
        "env = DummyVecEnv([lambda: gym.make(\"Pendulum-v0\")])\n",
        "normalized_vec_env = VecNormalize(env)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PAbu21pg90A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4770ac67-3062-4956-e614-cbd94d4b50dd"
      },
      "source": [
        "obs = normalized_vec_env.reset()\n",
        "for _ in range(10):\n",
        "  action = [normalized_vec_env.action_space.sample()]\n",
        "  obs, reward, _, _ = normalized_vec_env.step(action)\n",
        "  print(obs, reward)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.54705644 -0.12768991  0.9998632 ]] [-10.]\n",
            "[[-1.3049014 -0.7458486  1.0757202]] [-2.0198078]\n",
            "[[-1.5993044 -1.4638258  1.3217945]] [-1.2708998]\n",
            "[[-1.7483342 -1.7895812  1.4796199]] [-0.99464875]\n",
            "[[-1.8062567 -1.9423994  1.4834249]] [-0.87714076]\n",
            "[[-1.8123285 -2.0375621  1.4577675]] [-0.8079896]\n",
            "[[-1.7793765 -2.1048453  1.447266 ]] [-0.758458]\n",
            "[[-1.6930909 -2.135008   1.3786477]] [-0.72255445]\n",
            "[[-1.5511655 -2.1546626  1.3992189]] [-0.68292195]\n",
            "[[-1.3239248 -2.11952    1.2115979]] [-0.65953004]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEpTys28Wz05"
      },
      "source": [
        "## Exercise: code you own monitor wrapper\n",
        "\n",
        "Now that you know how does a wrapper work and what you can do with it, it's time to experiment.\n",
        "\n",
        "The goal here is to create a wrapper that will monitor the training progress, storing both the episode reward (sum of reward for one episode) and episode length (number of steps in for the last episode).\n",
        "\n",
        "You will return those values using the `info` dict after each end of episode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FWeDRd5W7hO"
      },
      "source": [
        "class MyMonitorWrapper(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  :param env: (gym.Env) Gym environment that will be wrapped\n",
        "  \"\"\"\n",
        "  def __init__(self, env):\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super(MyMonitorWrapper, self).__init__(env)\n",
        "    # === YOUR CODE HERE ===#\n",
        "    # Initialize the variables that will be used\n",
        "    # to store the episode length and episode reward\n",
        "\n",
        "    # ====================== #\n",
        "  \n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment \n",
        "    \"\"\"\n",
        "    obs = self.env.reset()\n",
        "    # === YOUR CODE HERE ===#\n",
        "    # Reset the variables\n",
        "\n",
        "    # ====================== #\n",
        "    return obs\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    :param action: ([float] or int) Action taken by the agent\n",
        "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
        "    \"\"\"\n",
        "    obs, reward, done, info = self.env.step(action)\n",
        "    # === YOUR CODE HERE ===#\n",
        "    # Update the current episode reward and episode length\n",
        "\n",
        "    # ====================== #\n",
        "\n",
        "    if done:\n",
        "      # === YOUR CODE HERE ===#\n",
        "      # Store the episode length and episode reward in the info dict\n",
        "      print(info)  # added just to see what it looks like before modifying\n",
        "      # ====================== #\n",
        "    \n",
        "    return obs, reward, done, info"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4fY4QwWXNFK"
      },
      "source": [
        "#### Test your wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJbUG-A_liYt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08b3ee40-77a5-4a3e-a365-5f55068dc908"
      },
      "source": [
        "# To use LunarLander, you need to install box2d box2d-kengz (pip) and swig (apt-get)\n",
        "!pip install box2d box2d-kengz"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting box2d\n",
            "  Downloading Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 14.0 MB/s \n",
            "\u001b[?25hCollecting box2d-kengz\n",
            "  Downloading Box2D-kengz-2.3.3.tar.gz (425 kB)\n",
            "\u001b[K     |████████████████████████████████| 425 kB 70.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-kengz\n",
            "  Building wheel for box2d-kengz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-kengz: filename=Box2D_kengz-2.3.3-cp37-cp37m-linux_x86_64.whl size=2053074 sha256=906863e3da1bf7d51ec2c5561b26948bac10d2cd3ca235f0050003ae9bd3f8ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/6d/6a/6ff76731fd9e8efbd1cdc6111e98b2dd0f1872184d7c28939c\n",
            "Successfully built box2d-kengz\n",
            "Installing collected packages: box2d-kengz, box2d\n",
            "Successfully installed box2d-2.3.10 box2d-kengz-2.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWZp1olSXMUg"
      },
      "source": [
        "env = gym.make(\"LunarLander-v2\")\n",
        "# === YOUR CODE HERE ===#\n",
        "# Wrap the environment\n",
        "\n",
        "# Reset the environment\n",
        "\n",
        "# Take random actions in the enviromnent and check\n",
        "# that it returns the correct values after the end of each episode\n",
        "\n",
        "# ====================== #"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLIdupbbEo1a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ2IqSM2eOt8"
      },
      "source": [
        " # Conclusion\n",
        " \n",
        " In this notebook, we have seen:\n",
        " - how to easily save and load a model\n",
        " - what is wrapper and what we can do with it\n",
        " - how to create your own wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhWB_bHpSkas"
      },
      "source": [
        "## Wrapper Bonus: changing the observation space: a wrapper for episode of fixed length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBlS9YxYSpJn"
      },
      "source": [
        "from gym.wrappers import TimeLimit\n",
        "\n",
        "class TimeFeatureWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Add remaining time to observation space for fixed length episodes.\n",
        "    See https://arxiv.org/abs/1712.00378 and https://github.com/aravindr93/mjrl/issues/13.\n",
        "\n",
        "    :param env: (gym.Env)\n",
        "    :param max_steps: (int) Max number of steps of an episode\n",
        "        if it is not wrapped in a TimeLimit object.\n",
        "    :param test_mode: (bool) In test mode, the time feature is constant,\n",
        "        equal to zero. This allow to check that the agent did not overfit this feature,\n",
        "        learning a deterministic pre-defined sequence of actions.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, max_steps=1000, test_mode=False):\n",
        "        assert isinstance(env.observation_space, gym.spaces.Box)\n",
        "        # Add a time feature to the observation\n",
        "        low, high = env.observation_space.low, env.observation_space.high\n",
        "        low, high= np.concatenate((low, [0])), np.concatenate((high, [1.]))\n",
        "        env.observation_space = gym.spaces.Box(low=low, high=high, dtype=np.float32)\n",
        "\n",
        "        super(TimeFeatureWrapper, self).__init__(env)\n",
        "\n",
        "        if isinstance(env, TimeLimit):\n",
        "            self._max_steps = env._max_episode_steps\n",
        "        else:\n",
        "            self._max_steps = max_steps\n",
        "        self._current_step = 0\n",
        "        self._test_mode = test_mode\n",
        "\n",
        "    def reset(self):\n",
        "        self._current_step = 0\n",
        "        return self._get_obs(self.env.reset())\n",
        "\n",
        "    def step(self, action):\n",
        "        self._current_step += 1\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        return self._get_obs(obs), reward, done, info\n",
        "\n",
        "    def _get_obs(self, obs):\n",
        "        \"\"\"\n",
        "        Concatenate the time feature to the current observation.\n",
        "\n",
        "        :param obs: (np.ndarray)\n",
        "        :return: (np.ndarray)\n",
        "        \"\"\"\n",
        "        # Remaining time is more general\n",
        "        time_feature = 1 - (self._current_step / self._max_steps)\n",
        "        if self._test_mode:\n",
        "            time_feature = 1.0\n",
        "        # Optionnaly: concatenate [time_feature, time_feature ** 2]\n",
        "        return np.concatenate((obs, [time_feature]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-vWgkZzd4F1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojn4nvNNRUoT"
      },
      "source": [
        "## Going further - Saving format \n",
        "\n",
        "The format for saving and loading models is a zip-archived JSON dump and NumPy zip archive of the arrays:\n",
        "```\n",
        "saved_model.zip/\n",
        "├── data              JSON file of class-parameters (dictionary)\n",
        "├── parameter_list    JSON file of model parameters and their ordering (list)\n",
        "├── parameters        Bytes from numpy.savez (a zip file of the numpy arrays). ...\n",
        "    ├── ...           Being a zip-archive itself, this object can also be opened ...\n",
        "        ├── ...       as a zip-archive and browsed.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWAcc8RFRUoU"
      },
      "source": [
        "## Save and find "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tcQxzSCRUoV"
      },
      "source": [
        "# Create save dir\n",
        "save_dir = \"/tmp/gym/\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "model = PPO('MlpPolicy', 'Pendulum-v0', verbose=0).learn(8000)\n",
        "model.save(save_dir + \"/PPO_tutorial\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGaMNz4HRUoX"
      },
      "source": [
        "!ls /tmp/gym/PPO_tutorial*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYY3nQyyRUoa"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "archive = zipfile.ZipFile(\"/tmp/gym/PPO_tutorial.zip\", 'r')\n",
        "for f in archive.filelist:\n",
        "  print(f.filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPKkkTvjRUo2"
      },
      "source": [
        "## Exporting saved models\n",
        "\n",
        "And finally some futher reading for those who want to export to tensorflowJS or Java.\n",
        "\n",
        "https://stable-baselines.readthedocs.io/en/master/guide/export.html"
      ]
    }
  ]
}