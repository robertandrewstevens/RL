{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_q_network_breakout",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2Ph6dJLvQMc"
      },
      "source": [
        "# Deep Q-Learning for Atari Breakout\n",
        "\n",
        "**Author:** [Jacob Chapman](https://twitter.com/jacoblchapman) and [Mathias Lechner](https://twitter.com/MLech20)<br>\n",
        "**Date created:** 2020/05/23<br>\n",
        "**Last modified:** 2020/06/17<br>\n",
        "**Description:** Play Atari Breakout with a Deep Q-Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPdD4X-yvQMi"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This script shows an implementation of Deep Q-Learning on the\n",
        "`BreakoutNoFrameskip-v4` environment.\n",
        "\n",
        "### Deep Q-Learning\n",
        "\n",
        "As an agent takes actions and moves through an environment, it learns to map\n",
        "the observed state of the environment to an action. An agent will choose an action\n",
        "in a given state based on a \"Q-value\", which is a weighted reward based on the\n",
        "expected highest long-term reward. A Q-Learning Agent learns to perform its\n",
        "task such that the recommended action maximizes the potential future rewards.\n",
        "This method is considered an \"Off-Policy\" method,\n",
        "meaning its Q values are updated assuming that the best action was chosen, even\n",
        "if the best action was not chosen.\n",
        "\n",
        "### Atari Breakout\n",
        "\n",
        "In this environment, a board moves along the bottom of the screen returning a ball that\n",
        "will destroy blocks at the top of the screen.\n",
        "The aim of the game is to remove all blocks and breakout of the\n",
        "level. The agent must learn to control the board by moving left and right, returning the\n",
        "ball and removing all the blocks without the ball passing the board.\n",
        "\n",
        "### Note\n",
        "\n",
        "The Deepmind paper trained for \"a total of 50 million frames (that is, around 38 days of\n",
        "game experience in total)\". However this script will give good results at around 10\n",
        "million frames which are processed in less than 24 hours on a modern machine.\n",
        "\n",
        "### References\n",
        "\n",
        "- [Q-Learning](https://link.springer.com/content/pdf/10.1007/BF00992698.pdf)\n",
        "- [Deep Q-Learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdcbUPczvQMj"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP3MR8iIvk8x",
        "outputId": "c734823f-5444-4d86-83c1-148d4bce5b76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# https://stackoverflow.com/questions/65338120/modulenotfounderror-no-module-named-baselines-common\n",
        "import sys\n",
        "!{sys.executable} -m pip install baselines"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting baselines\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/bd/d7695f0e5649658b43eabf10d1efa11c70a30ce532faef994c8b7172a744/baselines-0.1.5.tar.gz (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 22.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 61kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 81kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 102kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 112kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym[atari,classic_control,mujoco,robotics] in /usr/local/lib/python3.6/dist-packages (from baselines) (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from baselines) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from baselines) (4.41.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from baselines) (1.0.0)\n",
            "Collecting zmq\n",
            "  Downloading https://files.pythonhosted.org/packages/6e/78/833b2808793c1619835edb1a4e17a023d5d625f4f97ff25ffff986d1f472/zmq-0.0.0.tar.gz\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from baselines) (0.3.3)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from baselines) (3.38.0)\n",
            "Collecting mpi4py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/8f/bbd8de5ba566dd77e408d8136e2bab7fdf2b97ce06cab830ba8b50a2f588/mpi4py-3.0.3.tar.gz (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from baselines) (1.3.0)\n",
            "Requirement already satisfied: tensorflow>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from baselines) (2.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from baselines) (7.1.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control,mujoco,robotics]->baselines) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control,mujoco,robotics]->baselines) (1.19.4)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control,mujoco,robotics]->baselines) (7.0.0)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control,mujoco,robotics]->baselines) (4.1.2.30)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control,mujoco,robotics]->baselines) (0.2.6)\n",
            "Requirement already satisfied: imageio; extra == \"mujoco\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control,mujoco,robotics]->baselines) (2.4.1)\n",
            "Collecting mujoco-py<2.0,>=1.50; extra == \"mujoco\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/8c/64e0630b3d450244feef0688d90eab2448631e40ba6bdbd90a70b84898e7/mujoco-py-1.50.1.68.tar.gz (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from zmq->baselines) (20.0.0)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->baselines) (2.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from progressbar2->baselines) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (1.1.2)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (0.36.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (2.4.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (1.32.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (2.10.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (3.7.4.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (1.12.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (3.12.4)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (0.10.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (2.4.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.4.0->baselines) (1.6.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control,mujoco,robotics]->baselines) (0.16.0)\n",
            "Collecting glfw>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/72/3190b7cc8494c05d7fb350237d3f51abdaff79e74d1e13d6f84df4b57f6b/glfw-2.0.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 13.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"mujoco\"->gym[atari,classic_control,mujoco,robotics]->baselines) (0.29.21)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"mujoco\"->gym[atari,classic_control,mujoco,robotics]->baselines) (1.14.4)\n",
            "Collecting lockfile>=0.12.2\n",
            "  Downloading https://files.pythonhosted.org/packages/c8/22/9460e311f340cb62d26a38c419b1381b8593b0bb6b5d1f056938b086d362/lockfile-0.12.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow>=1.4.0->baselines) (51.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=1.4.0->baselines) (0.4.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=1.4.0->baselines) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=1.4.0->baselines) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=1.4.0->baselines) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=1.4.0->baselines) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=1.4.0->baselines) (1.0.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.10->mujoco-py<2.0,>=1.50; extra == \"mujoco\"->gym[atari,classic_control,mujoco,robotics]->baselines) (2.20)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (4.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (0.2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (2.10)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (3.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=1.4.0->baselines) (3.4.0)\n",
            "Building wheels for collected packages: baselines, zmq, mpi4py, mujoco-py\n",
            "  Building wheel for baselines (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for baselines: filename=baselines-0.1.5-cp36-none-any.whl size=163902 sha256=07bd0dbc34770bdeb1c52cc567465c041b38226b607328630bc3f13aaf7135ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/95/d4/dc08613e714458fb368c44948025dade91a37db5932faa058f\n",
            "  Building wheel for zmq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zmq: filename=zmq-0.0.0-cp36-none-any.whl size=1278 sha256=4c77f0824869fe687c63d9088a2f979ff9640d4fc3dd090aa145fe72c37a48dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/7a/7d/ac1d865766b06f9769ac1154bf31dbb5abb3b52ecfe278247a\n",
            "  Building wheel for mpi4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.0.3-cp36-cp36m-linux_x86_64.whl size=2074411 sha256=609bbb8273584b12e0356682b18f64f32d0b050be8739071090dbeef86a0da0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/e0/86/2b713dd512199096012ceca61429e12b960888de59818871d6\n",
            "  Building wheel for mujoco-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for mujoco-py\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for mujoco-py\n",
            "Successfully built baselines zmq mpi4py\n",
            "Failed to build mujoco-py\n",
            "Installing collected packages: zmq, mpi4py, baselines, glfw, lockfile, mujoco-py\n",
            "    Running setup.py install for mujoco-py ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-v8235747/mujoco-py/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-v8235747/mujoco-py/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-61_934wl/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtN5p8xqvQMj",
        "outputId": "9594f4a5-c816-40cb-9b10-7c63592c86b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Configuration paramaters for the whole setup\n",
        "seed = 42\n",
        "gamma = 0.99  # Discount factor for past rewards\n",
        "epsilon = 1.0  # Epsilon greedy parameter\n",
        "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
        "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
        "epsilon_interval = (\n",
        "    epsilon_max - epsilon_min\n",
        ")  # Rate at which to reduce chance of random action being taken\n",
        "batch_size = 32  # Size of batch taken from replay buffer\n",
        "max_steps_per_episode = 10000\n",
        "\n",
        "# Use the Baseline Atari environment because of Deepmind helper functions\n",
        "env = make_atari(\"BreakoutNoFrameskip-v4\")\n",
        "# Warp the frames, grey scale, stake four frame and scale to smaller ratio\n",
        "env = wrap_deepmind(env, frame_stack=True, scale=True)\n",
        "env.seed(seed)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[42, 742738649]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YMYJUbOvQMk"
      },
      "source": [
        "## Implement the Deep Q-Network\n",
        "\n",
        "This network learns an approximation of the Q-table, which is a mapping between\n",
        "the states and actions that an agent will take. For every state we'll have four\n",
        "actions, that can be taken. The environment provides the state, and the action\n",
        "is chosen by selecting the larger of the four Q-values predicted in the output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iITjJcMKvQMk"
      },
      "source": [
        "num_actions = 4\n",
        "\n",
        "\n",
        "def create_q_model():\n",
        "    # Network defined by the Deepmind paper\n",
        "    inputs = layers.Input(shape=(84, 84, 4,))\n",
        "\n",
        "    # Convolutions on the frames on the screen\n",
        "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
        "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
        "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
        "\n",
        "    layer4 = layers.Flatten()(layer3)\n",
        "\n",
        "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
        "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=action)\n",
        "\n",
        "\n",
        "# The first model makes the predictions for Q-values which are used to\n",
        "# make a action.\n",
        "model = create_q_model()\n",
        "# Build a target model for the prediction of future rewards.\n",
        "# The weights of a target model get updated every 10000 steps thus when the\n",
        "# loss between the Q-values is calculated the target Q-value is stable.\n",
        "model_target = create_q_model()\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLrl2C0gvQMl"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNv7L1FvQMl",
        "outputId": "b9814dee-eb3c-4565-d3c4-bff2f36a9c9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
        "# improves training time\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
        "\n",
        "# Experience replay buffers\n",
        "action_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "done_history = []\n",
        "episode_reward_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "frame_count = 0\n",
        "# Number of frames to take random action and observe output\n",
        "epsilon_random_frames = 50000\n",
        "# Number of frames for exploration\n",
        "epsilon_greedy_frames = 1000000.0\n",
        "# Maximum replay length\n",
        "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
        "max_memory_length = 100000\n",
        "# Train the model after 4 actions\n",
        "update_after_actions = 4\n",
        "# How often to update the target network\n",
        "update_target_network = 10000\n",
        "# Using huber loss for stability\n",
        "loss_function = keras.losses.Huber()\n",
        "\n",
        "while True:  # Run until solved\n",
        "    state = np.array(env.reset())\n",
        "    episode_reward = 0\n",
        "\n",
        "    for timestep in range(1, max_steps_per_episode):\n",
        "        # env.render(); Adding this line would show the attempts\n",
        "        # of the agent in a pop up window.\n",
        "        frame_count += 1\n",
        "\n",
        "        # Use epsilon-greedy for exploration\n",
        "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
        "            # Take random action\n",
        "            action = np.random.choice(num_actions)\n",
        "        else:\n",
        "            # Predict action Q-values\n",
        "            # From environment state\n",
        "            state_tensor = tf.convert_to_tensor(state)\n",
        "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
        "            action_probs = model(state_tensor, training=False)\n",
        "            # Take best action\n",
        "            action = tf.argmax(action_probs[0]).numpy()\n",
        "\n",
        "        # Decay probability of taking random action\n",
        "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
        "        epsilon = max(epsilon, epsilon_min)\n",
        "\n",
        "        # Apply the sampled action in our environment\n",
        "        state_next, reward, done, _ = env.step(action)\n",
        "        state_next = np.array(state_next)\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Save actions and states in replay buffer\n",
        "        action_history.append(action)\n",
        "        state_history.append(state)\n",
        "        state_next_history.append(state_next)\n",
        "        done_history.append(done)\n",
        "        rewards_history.append(reward)\n",
        "        state = state_next\n",
        "\n",
        "        # Update every fourth frame and once batch size is over 32\n",
        "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
        "\n",
        "            # Get indices of samples for replay buffers\n",
        "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
        "\n",
        "            # Using list comprehension to sample from replay buffer\n",
        "            state_sample = np.array([state_history[i] for i in indices])\n",
        "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
        "            rewards_sample = [rewards_history[i] for i in indices]\n",
        "            action_sample = [action_history[i] for i in indices]\n",
        "            done_sample = tf.convert_to_tensor(\n",
        "                [float(done_history[i]) for i in indices]\n",
        "            )\n",
        "\n",
        "            # Build the updated Q-values for the sampled future states\n",
        "            # Use the target model for stability\n",
        "            future_rewards = model_target.predict(state_next_sample)\n",
        "            # Q value = reward + discount factor * expected future reward\n",
        "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
        "                future_rewards, axis=1\n",
        "            )\n",
        "\n",
        "            # If final frame set the last value to -1\n",
        "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
        "\n",
        "            # Create a mask so we only calculate loss on the updated Q-values\n",
        "            masks = tf.one_hot(action_sample, num_actions)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Train the model on the states and updated Q-values\n",
        "                q_values = model(state_sample)\n",
        "\n",
        "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
        "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "                # Calculate loss between new Q-value and old Q-value\n",
        "                loss = loss_function(updated_q_values, q_action)\n",
        "\n",
        "            # Backpropagation\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        if frame_count % update_target_network == 0:\n",
        "            # update the the target network with new weights\n",
        "            model_target.set_weights(model.get_weights())\n",
        "            # Log details\n",
        "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
        "            print(template.format(running_reward, episode_count, frame_count))\n",
        "\n",
        "        # Limit the state and reward history\n",
        "        if len(rewards_history) > max_memory_length:\n",
        "            del rewards_history[:1]\n",
        "            del state_history[:1]\n",
        "            del state_next_history[:1]\n",
        "            del action_history[:1]\n",
        "            del done_history[:1]\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Update running reward to check condition for solving\n",
        "    episode_reward_history.append(episode_reward)\n",
        "    if len(episode_reward_history) > 100:\n",
        "        del episode_reward_history[:1]\n",
        "    running_reward = np.mean(episode_reward_history)\n",
        "\n",
        "    episode_count += 1\n",
        "\n",
        "    if running_reward > 40:  # Condition to consider the task solved\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running reward: 0.34 at episode 297, frame count 10000\n",
            "running reward: 0.30 at episode 599, frame count 20000\n",
            "running reward: 0.28 at episode 869, frame count 30000\n",
            "running reward: 0.23 at episode 1159, frame count 40000\n",
            "running reward: 0.33 at episode 1461, frame count 50000\n",
            "running reward: 0.34 at episode 1757, frame count 60000\n",
            "running reward: 0.31 at episode 2050, frame count 70000\n",
            "running reward: 0.26 at episode 2342, frame count 80000\n",
            "running reward: 0.32 at episode 2640, frame count 90000\n",
            "running reward: 0.28 at episode 2939, frame count 100000\n",
            "running reward: 0.33 at episode 3221, frame count 110000\n",
            "running reward: 0.38 at episode 3510, frame count 120000\n",
            "running reward: 0.30 at episode 3785, frame count 130000\n",
            "running reward: 0.40 at episode 4042, frame count 140000\n",
            "running reward: 0.40 at episode 4299, frame count 150000\n",
            "running reward: 0.46 at episode 4530, frame count 160000\n",
            "running reward: 0.48 at episode 4765, frame count 170000\n",
            "running reward: 0.44 at episode 5008, frame count 180000\n",
            "running reward: 0.55 at episode 5213, frame count 190000\n",
            "running reward: 0.61 at episode 5430, frame count 200000\n",
            "running reward: 0.52 at episode 5668, frame count 210000\n",
            "running reward: 0.59 at episode 5887, frame count 220000\n",
            "running reward: 0.54 at episode 6115, frame count 230000\n",
            "running reward: 0.43 at episode 6376, frame count 240000\n",
            "running reward: 0.43 at episode 6627, frame count 250000\n",
            "running reward: 0.29 at episode 6894, frame count 260000\n",
            "running reward: 0.49 at episode 7128, frame count 270000\n",
            "running reward: 0.73 at episode 7335, frame count 280000\n",
            "running reward: 0.77 at episode 7521, frame count 290000\n",
            "running reward: 0.56 at episode 7729, frame count 300000\n",
            "running reward: 0.61 at episode 7948, frame count 310000\n",
            "running reward: 0.65 at episode 8152, frame count 320000\n",
            "running reward: 0.82 at episode 8343, frame count 330000\n",
            "running reward: 0.62 at episode 8552, frame count 340000\n",
            "running reward: 0.60 at episode 8767, frame count 350000\n",
            "running reward: 0.83 at episode 8956, frame count 360000\n",
            "running reward: 0.54 at episode 9177, frame count 370000\n",
            "running reward: 0.87 at episode 9357, frame count 380000\n",
            "running reward: 0.85 at episode 9542, frame count 390000\n",
            "running reward: 0.81 at episode 9741, frame count 400000\n",
            "running reward: 0.62 at episode 9935, frame count 410000\n",
            "running reward: 1.06 at episode 10107, frame count 420000\n",
            "running reward: 0.61 at episode 10316, frame count 430000\n",
            "running reward: 0.74 at episode 10515, frame count 440000\n",
            "running reward: 0.85 at episode 10697, frame count 450000\n",
            "running reward: 1.05 at episode 10851, frame count 460000\n",
            "running reward: 0.84 at episode 11014, frame count 470000\n",
            "running reward: 1.12 at episode 11167, frame count 480000\n",
            "running reward: 1.08 at episode 11323, frame count 490000\n",
            "running reward: 1.04 at episode 11479, frame count 500000\n",
            "running reward: 1.00 at episode 11638, frame count 510000\n",
            "running reward: 1.30 at episode 11780, frame count 520000\n",
            "running reward: 1.36 at episode 11922, frame count 530000\n",
            "running reward: 1.62 at episode 12041, frame count 540000\n",
            "running reward: 1.74 at episode 12170, frame count 550000\n",
            "running reward: 1.49 at episode 12305, frame count 560000\n",
            "running reward: 1.31 at episode 12439, frame count 570000\n",
            "running reward: 1.72 at episode 12555, frame count 580000\n",
            "running reward: 1.68 at episode 12676, frame count 590000\n",
            "running reward: 1.61 at episode 12798, frame count 600000\n",
            "running reward: 1.70 at episode 12918, frame count 610000\n",
            "running reward: 1.71 at episode 13039, frame count 620000\n",
            "running reward: 2.15 at episode 13141, frame count 630000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMTsro1fvQMo"
      },
      "source": [
        "## Visualizations\n",
        "Before any training:\n",
        "![Imgur](https://i.imgur.com/rRxXF4H.gif)\n",
        "\n",
        "In early stages of training:\n",
        "![Imgur](https://i.imgur.com/X8ghdpL.gif)\n",
        "\n",
        "In later stages of training:\n",
        "![Imgur](https://i.imgur.com/Z1K6qBQ.gif)"
      ]
    }
  ]
}